{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting future energy usage from multiple dependent time series (v2)\n",
    "\n",
    "## Part 1: Load and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'doughudgeon-mlforbusiness' # change the name odf your bucket\n",
    "subfolder = 'ch06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd                               \n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# correspond to Version 2.x of the SageMaker Python SDK\n",
    "# Check the latest version of SageMaker\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Version is good\")\n",
    "else:\n",
    "    !{sys.executable} -m pip install --upgrade sagemaker\n",
    "    print(\"Installing latest SageMaker Version. Please restart the kernel\")\n",
    "    \n",
    "role = sagemaker.get_execution_role()\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = f\"s3://{data_bucket}/{subfolder}/data\"\n",
    "s3_output_path = f\"s3://{data_bucket}/{subfolder}/output\"\n",
    "df = pd.read_csv(f's3://{data_bucket}/{subfolder}/meter_data.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows in dataset: {df.shape[0]}')\n",
    "print(f'Number of columns in dataset: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Get the data in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "daily_df = df.resample('D').sum()\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_df.shape)\n",
    "print(f'Time series starts at {daily_df.index[0]} \\\n",
    "and ends at {daily_df.index[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = daily_df.fillna(daily_df.shift(7))\n",
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of time series:',daily_df.shape[1])\n",
    "fig, axs = plt.subplots(5, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "indices = [0,1,2,3,4,5,40,41,42,43]\n",
    "for i in indices:\n",
    "    plot_num = indices.index(i)\n",
    "    daily_df[daily_df.columns[i]].loc[\"2017-11-01\":\"2018-01-31\"].plot(ax=axx[plot_num])\n",
    "    axx[plot_num].set_xlabel(\"date\")    \n",
    "    axx[plot_num].set_ylabel(\"kW consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually there are some noticeable correlations which DeepAR will likely recognise and use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_power_consumption_per_site = []\n",
    "for column in daily_df.columns:\n",
    "    site_consumption = np.trim_zeros(daily_df[column], trim='f')\n",
    "    site_consumption = site_consumption.fillna(0)\n",
    "    daily_power_consumption_per_site.append(site_consumption)\n",
    "    \n",
    "print(f'Time series covers {len(daily_power_consumption_per_site[0])} days.')\n",
    "print(f'Time series starts at {daily_power_consumption_per_site[0].index[0]}')\n",
    "print(f'Time series ends at {daily_power_consumption_per_site[0].index[-1]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'D'\n",
    "prediction_length = 30\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "start_date = pd.Timestamp(\"2017-11-01 00:00:00\", freq=freq)\n",
    "end_training = start_date + datetime.timedelta(364)\n",
    "end_testing = end_training + datetime.timedelta(prediction_length)\n",
    "\n",
    "print(f'End training: {end_training}, End testing: {end_testing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_date),\n",
    "        \"target\": ts[start_date:end_training].tolist()\n",
    "    }\n",
    "    for ts in daily_power_consumption_per_site\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(start_date),\n",
    "        \"target\": ts[start_date:end_testing].tolist()\n",
    "    }\n",
    "    for ts in daily_power_consumption_per_site\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_s3(path, data):\n",
    "    with s3.open(path, 'wb') as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            f.write(\"\\n\".encode('utf-8'))\n",
    "            \n",
    "write_dicts_to_s3(f'{s3_data_path}/train/train.json', training_data)\n",
    "write_dicts_to_s3(f'{s3_data_path}/test/test.json', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = f's3://{data_bucket}/{subfolder}/output'\n",
    "sess = sagemaker.Session()\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", sess.boto_region_name, \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge', # $0.476 per hour as of Jan 2019.\n",
    "    base_job_name='ch6-energy-usage',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    time_freq=freq,\n",
    "    epochs=\"400\",\n",
    "    early_stopping_patience=\"40\",\n",
    "    mini_batch_size=\"64\",\n",
    "    learning_rate=\"5E-4\",\n",
    "    context_length=\"90\",\n",
    "    prediction_length=str(prediction_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'energy-usage'\n",
    "\n",
    "try:\n",
    "    sess.delete_endpoint(\n",
    "        sagemaker.predictor.Predictor(endpoint=endpoint_name).endpoint, delete_endpoint_config=True)\n",
    "    print('Warning: Existing endpoint and configuration deleted to make way for your new endpoint.')\n",
    "    from time import sleep\n",
    "    sleep(30)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         #serializer=JSONSerializer(),\n",
    "                         serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "                         **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        \n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create an endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "#from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.serializers import CSVSerializer, IdentitySerializer\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    predictor_cls=DeepARPredictor,\n",
    "    #content_type='application/json',\n",
    "    #serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "    #serializer=JSONSerializer(),\n",
    "    #deserializer=JSONDeserializer(),\n",
    "    endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Make Predictions and Plot Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.predict(ts=daily_power_consumption_per_site[0][start_date+30:end_training], quantiles=[0.1, 0.5, 0.9]).head()\n",
    "# predictor.predict(ts=daily_power_consumption_per_site[0][start_date+datetime.timedelta(30)*start_date.freq:end_training],quantiles=[0.1, 0.5, 0.9]).head()\n",
    "predictor.predict(ts=daily_power_consumption_per_site[0][start_date+datetime.timedelta(30):end_training],quantiles=[0.1, 0.5, 0.9]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts,\n",
    "    end_training=end_training, \n",
    "    plot_weeks=12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(f\"Calling served model to generate predictions starting from {end_training} to {end_training+datetime.timedelta(prediction_length)}\")\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    plot_history = plot_weeks * 7\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    prediction = predictor.predict(ts=target_ts[:end_training], quantiles=[low_quantile, 0.5, up_quantile])\n",
    "                \n",
    "    target_section = target_ts[end_training-datetime.timedelta(plot_history):end_training+datetime.timedelta(prediction_length)]\n",
    "    target_section.plot(color=\"black\", label='Actual')\n",
    "    \n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "#     prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 33\n",
    "plot_weeks = 0\n",
    "confidence = 80\n",
    "plot(\n",
    "        predictor,\n",
    "        target_ts=daily_power_consumption_per_site[site_id][start_date+datetime.timedelta(30):],\n",
    "        plot_weeks=plot_weeks,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some objective statistics regarding accuracy of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE measures the \"root mean square\" error. It penalises more extreme \"misses\" and rewards consistency.\n",
    "It also has the advantage that it's magnititude is proportional to the value being predicted.\n",
    "MAPE measures \"Mean Absolute Percentage Error\". The main reason to use MAPE is that it scores errors in\n",
    "percentage terms rather than as absolutes. Hence a prediciton of 11 for a value of 10 is treated identically\n",
    "to a prediction of 90 for a value of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather 30 day predictions for all timeseries\n",
    "predictions= []\n",
    "for i, ts in enumerate(daily_power_consumption_per_site):\n",
    "\n",
    "    # call the end point to get the 30 day prediction\n",
    "    predictions.append(predictor.predict(ts=ts[start_date+datetime.timedelta(30):end_training])['0.5'].sum())\n",
    "\n",
    "usages = [ts[end_training+datetime.timedelta(1):end_training+datetime.timedelta(30)].sum() for ts in daily_power_consumption_per_site]\n",
    "\n",
    "for p,u in zip(predictions,usages):\n",
    "    print(f'Predicted {p} kwh but usage was {u} kwh,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MAPE: {round(mape(usages, predictions),1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Endpoint (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Endpoint (optional)\n",
    "# Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\"\n",
    "sagemaker.Session().delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
