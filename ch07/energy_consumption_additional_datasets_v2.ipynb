{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Dynamic Features To Improve Energy Usage Predictions (v2)\n",
    "\n",
    "## Part 1: Load and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'doughudgeon-mlforbusiness' # change the name odf your bucket\n",
    "subfolder = 'ch07'\n",
    "s3_data_path = f\"s3://{data_bucket}/{subfolder}/data\"\n",
    "s3_output_path = f\"s3://{data_bucket}/{subfolder}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd                               \n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# correspond to Version 2.x of the SageMaker Python SDK\n",
    "# Check the latest version of SageMaker\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Version is good\")\n",
    "else:\n",
    "    !{sys.executable} -m pip install --upgrade sagemaker\n",
    "    print(\"Installing latest SageMaker Version. Please restart the kernel\")\n",
    "    \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "s3_data_path = f\"s3://{data_bucket}/{subfolder}/data\"\n",
    "s3_output_path = f\"s3://{data_bucket}/{subfolder}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will be dealing with 5 files. Our meter readings have already been summarised into daily totals and can be found in \"Meter Data.csv\". We also have a \"Site Categories.csv\" file which records whether each site belongs to the Retail, Industrial, or Transport industries. We will use this as a static \"Category\" feature. There is a further file which contains time series data regarding holidays. This is \"Site Holidays.csv\". Finally we have maximum temperatures in \"Site Maxima.csv\" These will be our models \"dynamic features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we check our meter data\n",
    "daily_df = pd.read_csv(f's3://{data_bucket}/{subfolder}/meter_data_daily.csv', index_col=0, parse_dates=[0])\n",
    "daily_df.index.name = None\n",
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_df.shape)\n",
    "print(f'Time series starts at {daily_df.index[0]} and ends at {daily_df.index[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = pd.read_csv(f's3://{data_bucket}/{subfolder}/site_categories.csv',index_col=0).reset_index(drop=True)\n",
    "print(category_df.shape)\n",
    "print(category_df.Category.unique())\n",
    "category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = pd.read_csv(f's3://{data_bucket}/{subfolder}/site_holidays.csv', index_col=0, parse_dates=[0])\n",
    "print(holiday_df.shape)\n",
    "print(f'Time series starts at {holiday_df.index[0]} and ends at {holiday_df.index[-1]}')\n",
    "holiday_df.loc['2018-12-22':'2018-12-27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = pd.read_csv(f's3://{data_bucket}/{subfolder}/site_maximums.csv', index_col=0, parse_dates=[0])\n",
    "print(max_df.shape)\n",
    "print(f'Time series starts at {max_df.index[0]} and ends at {max_df.index[-1]}')\n",
    "max_df.loc['2018-12-22':'2018-12-27']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get the data in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do this to set freq='D' on the index:\n",
    "daily_df = daily_df.resample('D').sum()\n",
    "daily_df = daily_df.replace([0],[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df[daily_df.isnull().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sites have missing values for November 2017. We will not impute these but instead train with missing values.\n",
    "During the prediction step we will only use values from December. So daily_df is already in the right shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about categoricals?\n",
    "print(f'{len(category_df[category_df.isnull().any(axis=1)])} sites with missing categories.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic features?\n",
    "print(f'{len(holiday_df[holiday_df.isnull().any(axis=1)])} days with missing holidays.')\n",
    "print(f'{len(max_df[max_df.isnull().any(axis=1)])} days with missing maximum temperatures.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have to impute missing temperatures. Weather does not follow a weekly cycle like energu usage does,\n",
    "# but pandas has a very nice way to impute missing values for this very situation:\n",
    "max_df = max_df.interpolate(method='time')\n",
    "print(f'{len(max_df[max_df.isnull().any(axis=1)])} days with missing maximum temperatures. Problem solved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm visually we are dealing with the same or similar data to chapter 6:\n",
    "print('Number of time series:',daily_df.shape[1])\n",
    "fig, axs = plt.subplots(6, 2, figsize=(20, 20), sharex=True)\n",
    "axx = axs.ravel()\n",
    "indices = [0,1,2,3,26,27,33,39,42,43,46,47]\n",
    "for i in indices:\n",
    "    plot_num = indices.index(i)\n",
    "    daily_df[daily_df.columns[i]].loc[\"2017-11-01\":\"2019-02-28\"].plot(ax=axx[plot_num])\n",
    "    axx[plot_num].set_xlabel(\"date\")    \n",
    "    axx[plot_num].set_ylabel(\"kW consumption\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = list(category_df.Category.astype('category').cat.codes)\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = list(station_df['Station Number'].astype('category').cat.codes)\n",
    "# assert len(stations)==48\n",
    "# print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_per_site = [daily_df[col] for col in daily_df.columns]\n",
    "\n",
    "print(f'Time series covers {len(usage_per_site[0])} days.')\n",
    "print(f'Time series starts at {usage_per_site[0].index[0]}')\n",
    "print(f'Time series ends at {usage_per_site[0].index[-1]}') \n",
    "usage_per_site[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for our dynamic features\n",
    "hols_per_site = [holiday_df[col] for col in holiday_df.columns]\n",
    "    \n",
    "print(f'Time series covers {len(hols_per_site[0])} days.')\n",
    "print(f'Time series starts at {hols_per_site[0].index[0]}')\n",
    "print(f'Time series ends at {hols_per_site[0].index[-1]}') \n",
    "hols_per_site[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per_site = [max_df[col] for col in max_df.columns]\n",
    "    \n",
    "print(f'Time series covers {len(max_per_site[0])} days.')\n",
    "print(f'Time series starts at {max_per_site[0].index[0]}')\n",
    "print(f'Time series ends at {max_per_site[0].index[-1]}') \n",
    "max_per_site[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'D'\n",
    "prediction_length = 28\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "start_date = pd.Timestamp(\"2017-11-01\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2019-01-31\", freq=freq)\n",
    "end_testing = end_training + timedelta(days=prediction_length)\n",
    "\n",
    "print(f'End training: {end_training}, End testing: {end_testing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_s3(path, data):\n",
    "    with s3.open(path, 'wb') as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            f.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We have missing values in ts for November only.\n",
    "#       Dynamic features must have numeric values for every entry.\n",
    "training_data = [\n",
    "    {\n",
    "        \"cat\": [cat],\n",
    "        \"start\": str(start_date),\n",
    "        \"target\": ts[start_date:end_training].tolist(),\n",
    "        \"dynamic_feat\": [\n",
    "            hols[start_date:end_training].tolist(),\n",
    "            maxes[start_date:end_training].tolist(),\n",
    "        ] # Note: List of lists\n",
    "    }\n",
    "    for cat,ts,hols,maxes in zip(cats, usage_per_site, hols_per_site, max_per_site)\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"cat\": [cat],\n",
    "        \"start\": str(start_date),\n",
    "        \"target\": ts[start_date:end_testing].tolist(),\n",
    "        \"dynamic_feat\": [\n",
    "            hols[start_date:end_testing].tolist(),\n",
    "            maxes[start_date:end_testing].tolist(),\n",
    "        ] # Note: List of lists\n",
    "    }\n",
    "    for cat,ts,hols,maxes in zip(cats, usage_per_site, hols_per_site, max_per_site)\n",
    "]\n",
    "            \n",
    "write_dicts_to_s3(f'{s3_data_path}/train/train.json', training_data)\n",
    "write_dicts_to_s3(f'{s3_data_path}/test/test.json', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Set up session and configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = f's3://{data_bucket}/{subfolder}/output'\n",
    "sess = sagemaker.Session()\n",
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", sess.boto_region_name, \"latest\")\n",
    "\n",
    "data_channels = {\n",
    "    \"train\": f\"{s3_data_path}/train/\",\n",
    "    \"test\": f\"{s3_data_path}/test/\"\n",
    "}\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the class from Chapter 6:\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, \n",
    "                         #serializer=JSONSerializer(),\n",
    "                         serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "                         **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        \n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5a: Create model without using additional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will establish a baseline without categorical or dynamic features. Note that this cell is commented out as you only need to run it if you want to see the MAPE without incorporating additional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = sagemaker.estimator.Estimator(\n",
    "#     sagemaker_session=sess,\n",
    "#     image_name=image_name,\n",
    "#     role=role,\n",
    "#     train_instance_count=1,\n",
    "#     train_instance_type='ml.c5.2xlarge', # $0.476 per hour as of Jan 2019.\n",
    "#     base_job_name='ch7-energy-usage-baseline',\n",
    "#     output_path=s3_output_path\n",
    "# )\n",
    "\n",
    "# estimator.set_hyperparameters(\n",
    "#     cardinality='ignore', # DISABLES CATEGORICALS FOR BASELINE\n",
    "#     context_length=\"90\",\n",
    "#     prediction_length=str(prediction_length),\n",
    "#     time_freq=freq,\n",
    "#     epochs=\"400\",\n",
    "#     early_stopping_patience=\"40\",\n",
    "#     mini_batch_size=\"64\",\n",
    "#     learning_rate=\"5E-4\",\n",
    "#     num_dynamic_feat=\"ignore\", # DISABLE DYNAMIC FEATURES FOR BASELINE\n",
    "# )\n",
    "\n",
    "# estimator.fit(inputs=data_channels, wait=True)\n",
    "\n",
    "# endpoint_name = 'energy-usage-baseline'\n",
    "\n",
    "# try:\n",
    "#     sess.delete_endpoint(\n",
    "#         sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name).endpoint)\n",
    "#     print('Warning: Existing endpoint and configuration deleted to make way for your new endpoint.')\n",
    "#     from time import sleep\n",
    "#     sleep(30)\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# predictor = estimator.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type='ml.m5.large',\n",
    "#     predictor_cls=DeepARPredictor,\n",
    "#     endpoint_name=endpoint_name)\n",
    "\n",
    "# # Gather 28 day predictions for all timeseries\n",
    "# usages = [ts[end_training+1:end_training+28].sum() for ts in usage_per_site]\n",
    "\n",
    "# predictions= []\n",
    "# for s in range(len(usage_per_site)):\n",
    "#     # call the end point to get the 28 day prediction\n",
    "#     predictions.append(\n",
    "#         predictor.predict(\n",
    "#             ts=usage_per_site[s][start_date+30:end_training],\n",
    "#         )['0.5'].sum()\n",
    "#     )\n",
    "    \n",
    "# print(f'MAPE: {round(mape(usages, predictions),1)}%')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5b: Model incorporating additional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_name,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge', # $0.476 per hour as of Jan 2019.\n",
    "    base_job_name='ch7-energy-usage-dynamic',\n",
    "    output_path=s3_output_path\n",
    ")\n",
    "\n",
    "estimator.set_hyperparameters(\n",
    "    context_length=\"90\",\n",
    "    prediction_length=str(prediction_length),\n",
    "    time_freq=freq,\n",
    "    epochs=\"400\",\n",
    "    early_stopping_patience=\"40\",\n",
    "    mini_batch_size=\"64\",\n",
    "    learning_rate=\"5E-4\",\n",
    "    num_dynamic_feat=2,\n",
    ")\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Making predictions from the model that incorporates additional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'energy-usage-dynamic'\n",
    "\n",
    "try:\n",
    "    sess.delete_endpoint(\n",
    "        sagemaker.predictor.Predictor(endpoint=endpoint_name).endpoint)\n",
    "    print('Warning: Existing endpoint and configuration deleted to make way for your new endpoint.')\n",
    "    from time import sleep\n",
    "    sleep(30)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    predictor_cls=DeepARPredictor,\n",
    "    endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction: (Delete endpoint configuration if retrying)\n",
    "predictor.predict(\n",
    "    cat=[cats[0]],\n",
    "    ts=usage_per_site[0][start_date+datetime.timedelta(30):end_training],\n",
    "     dynamic_feat=[\n",
    "             hols_per_site[0][start_date+datetime.timedelta(30):end_training+datetime.timedelta(28)].tolist(),\n",
    "             max_per_site[0][start_date+datetime.timedelta(30):end_training+datetime.timedelta(28)].tolist(),\n",
    "         ],\n",
    "    quantiles=[0.1, 0.5, 0.9]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather 28 day predictions for all timeseries\n",
    "usages = [ts[end_training+datetime.timedelta(1):end_training+datetime.timedelta(28)].sum() for ts in usage_per_site]\n",
    "\n",
    "predictions= []\n",
    "for s in range(len(usage_per_site)):\n",
    "    # call the end point to get the 28 day prediction\n",
    "    predictions.append(\n",
    "        predictor.predict(\n",
    "            cat=[cats[s]],\n",
    "            ts=usage_per_site[s][start_date+datetime.timedelta(30):end_training],\n",
    "            dynamic_feat=[\n",
    "                hols_per_site[s][start_date+datetime.timedelta(30):end_training+datetime.timedelta(28)].tolist(),\n",
    "                max_per_site[s][start_date+datetime.timedelta(30):end_training+datetime.timedelta(28)].tolist(),\n",
    "             ]\n",
    "        )['0.5'].sum()\n",
    "    )\n",
    "\n",
    "for p,u in zip(predictions,usages):\n",
    "    print(f'Predicted {p} kwh but usage was {u} kwh.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MAPE: {round(mape(usages, predictions),1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a really impressive improvement from 21%. What does this look like visually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    site_id,\n",
    "    end_training=end_training, \n",
    "    plot_weeks=12,\n",
    "    confidence=80\n",
    "):\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "    target_ts = usage_per_site[site_id][start_date+datetime.timedelta(30):]\n",
    "    dynamic_feats = [\n",
    "            hols_per_site[site_id][start_date+datetime.timedelta(30):].tolist(),\n",
    "            max_per_site[site_id][start_date+datetime.timedelta(30):].tolist(),\n",
    "        ]\n",
    "        \n",
    "    plot_history = plot_weeks * 7\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    prediction = predictor.predict(\n",
    "        cat = [cats[site_id]],\n",
    "        ts=target_ts[:end_training],\n",
    "        dynamic_feat=dynamic_feats,\n",
    "        quantiles=[low_quantile, 0.5, up_quantile])\n",
    "                \n",
    "    target_section = target_ts[end_training-datetime.timedelta(plot_history):end_training+datetime.timedelta(prediction_length)]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )  \n",
    "    \n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4 of each category:\n",
    "indices = [2,26,33,39,42,47,3]\n",
    "for i in indices:\n",
    "    plot_num = indices.index(i)\n",
    "    plot(\n",
    "        predictor,\n",
    "        site_id=i,\n",
    "        plot_weeks=6,\n",
    "        confidence=80\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Remove the Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Endpoint\n",
    "# Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\"\n",
    "\n",
    "#sess.delete_endpoint('energy-usage-baseline')\n",
    "sess.delete_endpoint('energy-usage-dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
