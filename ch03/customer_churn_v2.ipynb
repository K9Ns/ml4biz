{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting whether to contact a customer because they are at risk of churning (v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For updates on the way Sagemaker or AWS behave compared to the notebook code, please refer to https://livebook.manning.com/#!/book/machine-learning-for-business/chapter-3/v-5/119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = 'doughudgeon-mlforbusiness' # change the name odf your bucket\n",
    "subfolder = 'ch03'\n",
    "dataset = 'churn_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import s3fs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# correspond to Version 2.x of the SageMaker Python SDK\n",
    "# Check the latest version of SageMaker\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Version is good\")\n",
    "else:\n",
    "    !{sys.executable} -m pip install --upgrade sagemaker\n",
    "    print(\"Installing latest SageMaker Version. Please restart the kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f's3://{data_bucket}/{subfolder}/{dataset}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows in dataset: {df.shape[0]}')\n",
    "print(df['churned'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get the data into the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns.tolist()\n",
    "encoded_data = df.drop(['id', 'customer_code', 'co_name'], axis=1)\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create training, validation and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoded_data['churned']\n",
    "train_df, test_and_val_data, _, _ = train_test_split(encoded_data, y, test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "y = test_and_val_data['churned']\n",
    "val_df, test_df, _, _ = train_test_split(test_and_val_data, y, test_size=0.333, stratify=y, random_state=0)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "print()\n",
    "print('Number of rows in Train dataset: {train_df.shape[0]}')\n",
    "print(train_df['churned'].value_counts())\n",
    "print()\n",
    "print('Number of rows in Validate dataset: {val_df.shape[0]}')\n",
    "print(val_df['churned'].value_counts())\n",
    "print()\n",
    "print('Number of rows in Test dataset: {test_df.shape[0]}')\n",
    "print(test_df['churned'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.to_csv(None, header=False, index=False).encode()\n",
    "val_data = val_df.to_csv(None, header=False, index=False).encode()\n",
    "test_data = test_df.to_csv(None, header=True, index=False).encode()\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/train.csv', 'wb') as f:\n",
    "    f.write(train_data)\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/val.csv', 'wb') as f:\n",
    "    f.write(val_data) \n",
    "    \n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/test.csv', 'wb') as f:\n",
    "    f.write(test_data) \n",
    "    \n",
    "train_input = sagemaker.TrainingInput(s3_data=f's3://{data_bucket}/{subfolder}/processed/train.csv', content_type='csv')\n",
    "val_input = sagemaker.TrainingInput(s3_data=f's3://{data_bucket}/{subfolder}/processed/val.csv', content_type='csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "                'xgboost',\n",
    "                boto3.Session().region_name,\n",
    "                'latest')\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "                        container, \n",
    "                        role,\n",
    "                        instance_count=1, \n",
    "                        instance_type='ml.m4.xlarge',\n",
    "                        output_path=f's3://{data_bucket}/{subfolder}/output',\n",
    "                        sagemaker_session=sess)\n",
    "\n",
    "estimator.set_hyperparameters(\n",
    "                        max_depth=3,\n",
    "                        subsample=0.7,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric='auc',\n",
    "                        num_round=100,\n",
    "                        early_stopping_rounds=10,\n",
    "                        scale_pos_weight=17)\n",
    "\n",
    "estimator.fit({'train': train_input, 'validation': val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Host the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'customer-churn'\n",
    "\n",
    "try:\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    print('Warning: Existing endpoint deleted to make way for your new endpoint.')\n",
    "    sleep(30)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "                initial_instance_count=1,\n",
    "                instance_type='ml.m4.xlarge', \n",
    "                endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(row):\n",
    "    prob = float(predictor.predict(row[1:]).decode('utf-8'))\n",
    "    return 1 if prob > 0.5 else 0\n",
    "\n",
    "with s3.open(f'{data_bucket}/{subfolder}/processed/test.csv') as f:\n",
    "    test_data = pd.read_csv(f)\n",
    "\n",
    "test_data['prediction'] = test_data.apply(get_prediction, axis=1)\n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data['churned'].value_counts())\n",
    "print(test_data['prediction'].value_counts())\n",
    "print(metrics.accuracy_score(test_data['churned'],test_data['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(test_data['churned'],test_data['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1,0,0,0,0,0,0,0,0,2]\n",
    "pred = [0,0,0,0,0,0,0,0,1,2]\n",
    "print(metrics.confusion_matrix(y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Endpoint (optional)\n",
    "Comment out this cell to remove the endpoint if you want the endpoint to exist after \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
